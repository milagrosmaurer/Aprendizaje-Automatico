{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "31pceL-iwfYz",
        "CZ8fuRwIp1hn",
        "55DaPLK8p5VD",
        "TSWNUrgdwMqI"
      ],
      "authorship_tag": "ABX9TyOPtSjqWq0sGbD+Q1mRQoqy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/milagrosmaurer/Aprendizaje-Automatico/blob/main/TP2/TP2_automatico.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Carga de librerias"
      ],
      "metadata": {
        "id": "H78qTC9BlZMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ebooklib beautifulsoup4 pandas"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9Wct7dMGqvR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJ3TrxZflROM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset,Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from typing import List, Dict, Any\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ebooklib import epub\n",
        "import ebooklib\n",
        "from bs4 import BeautifulSoup\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creacion de datos"
      ],
      "metadata": {
        "id": "AsBnofU0ldDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"bert-base-multilingual-cased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "model.eval()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RKHUgiJplYsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text_X(t):\n",
        "    # Convertir a minúsculas y quitar puntuación\n",
        "    t = t.lower()\n",
        "    t = re.sub(r'[\\u200b-\\u200f\\uFEFF]', '', t)\n",
        "    t = re.sub(r\"[^a-záéíóúüñ0-9' -]+\", ' ', t)\n",
        "    t = re.sub(r'\\s+', ' ', t).strip()\n",
        "    return t"
      ],
      "metadata": {
        "id": "abGSiELDq-AG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text_y(t):\n",
        "    # Convertir a minúsculas y quitar puntuación\n",
        "    t = re.sub(r'[\\u200b-\\u200f\\uFEFF]', '', t)\n",
        "    t = re.sub(r\"[^a-zA-Záéíóúüñ0-9¿?,.' -]+\", ' ', t)\n",
        "    t = re.sub(r'\\s+', ' ', t).strip()\n",
        "    return t"
      ],
      "metadata": {
        "id": "L41yJLZ-rFBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transformar_etiqueta(y, indice):\n",
        "  puntuacion_iniciales = []\n",
        "  puntuacion_finales = []\n",
        "  capitalizaciones = []\n",
        "  instancia_ids = []\n",
        "  token_ids = []\n",
        "  tokens_l = []\n",
        "\n",
        "  for parrafo in y:\n",
        "    inicio_pregunta = False\n",
        "    palabras = parrafo.split()\n",
        "\n",
        "\n",
        "    for palabra in palabras:\n",
        "      tokens = tokenizer.tokenize(palabra.lower())\n",
        "\n",
        "      for i in range(len(tokens)):\n",
        "        if tokens[i] == \"¿\":\n",
        "          inicio_pregunta = True\n",
        "          continue\n",
        "        if tokens[i] == \"?\" or tokens[i] == \".\" or tokens[i] == \",\":\n",
        "          continue\n",
        "\n",
        "        instancia_ids.append(indice)\n",
        "        token_ids.append(tokenizer.convert_tokens_to_ids(tokens[i]))\n",
        "        tokens_l.append(tokens[i])\n",
        "\n",
        "        if inicio_pregunta:\n",
        "          puntuacion_iniciales.append(1) #('\"¿\"')\n",
        "          inicio_pregunta = False\n",
        "        else:\n",
        "          puntuacion_iniciales.append(0) #(\"\")\n",
        "\n",
        "        if i != len(tokens) - 1:\n",
        "\n",
        "          if tokens[i+1] == \"?\":\n",
        "            puntuacion_finales.append(3) #('\"?\"')\n",
        "          elif tokens[i+1] == \".\":\n",
        "            puntuacion_finales.append(1) #('\".\"')\n",
        "          elif tokens[i+1] == \",\":\n",
        "            puntuacion_finales.append(2) #('\",\"')\n",
        "          else:\n",
        "            puntuacion_finales.append(0) #(\"\")\n",
        "\n",
        "        else:\n",
        "          puntuacion_finales.append(0) #(\"\")\n",
        "\n",
        "        if palabra.islower():\n",
        "          capitalizaciones.append(0)\n",
        "          ultimo_numero = 0\n",
        "        elif palabra.istitle():\n",
        "          capitalizaciones.append(1)\n",
        "          ultimo_numero = 1\n",
        "        elif palabra.isupper():\n",
        "          capitalizaciones.append(3)\n",
        "          ultimo_numero = 3\n",
        "        else:\n",
        "          capitalizaciones.append(2)\n",
        "          ultimo_numero = 2\n",
        "\n",
        "  etiquetas = np.column_stack([instancia_ids, token_ids, tokens_l, puntuacion_iniciales, puntuacion_finales, capitalizaciones])\n",
        "  return etiquetas"
      ],
      "metadata": {
        "id": "JjpQPRzersC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convertir_epub_a_csv(archivo_epub='libro.epub'):\n",
        "  # Cargar el libro\n",
        "  book = ebooklib.epub.read_epub(archivo_epub)\n",
        "\n",
        "  # Lista donde se guardarán los párrafos\n",
        "  parrafos = []\n",
        "\n",
        "  # Recorremos los ítems del libro\n",
        "  for item in book.get_items():\n",
        "      if item.get_type() == ebooklib.ITEM_DOCUMENT:\n",
        "          # Parseamos el contenido HTML\n",
        "          soup = BeautifulSoup(item.get_body_content(), 'html.parser')\n",
        "          # Extraemos los párrafos\n",
        "          for p in soup.find_all('p'):\n",
        "            #print(\"p:\",p, 'tipo: ', type(p))\n",
        "            texto = p.get_text().strip()\n",
        "            #print(\"TEXTO:\",texto, ' tipo: ', type(texto))\n",
        "            palabras = texto.split()\n",
        "            #print(\"PALABRAS:\",palabras, ' tipo: ', type(palabras))\n",
        "            if len(palabras) < 20 or len(palabras) > 100:  # descartamos párrafos cortos\n",
        "                continue\n",
        "            if texto:\n",
        "                parrafos.append(texto)\n",
        "\n",
        "  df = pd.DataFrame({'parrafo': parrafos})\n",
        "  df.to_csv(\"libro_parrafos.csv\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "  print(f\"Se extrajeron {len(parrafos)} párrafos y se guardaron en 'libro_parrafos.csv'.\")"
      ],
      "metadata": {
        "id": "y48N-5J8s7We"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crearDataSet(libro):\n",
        "  convertir_epub_a_csv(libro)\n",
        "  df = pd.read_csv('libro_parrafos.csv')\n",
        "  parrafos = pd.DataFrame(columns=['default', 'limpio'])\n",
        "  parrafos['limpio'] = df['parrafo'].apply(normalize_text_X)\n",
        "  parrafos['default'] = df['parrafo'].apply(normalize_text_y)\n",
        "\n",
        "  columnas = ['instancia_id', 'token_id', 'token', 'punt_inicial', 'punt_final', 'capitalización']\n",
        "  datos = pd.DataFrame(columns=columnas)\n",
        "\n",
        "  i = 0\n",
        "  for p in parrafos['default']:\n",
        "    etiquetas = transformar_etiqueta([p], i)\n",
        "    etiquetas = pd.DataFrame(etiquetas, columns=columnas)\n",
        "    datos = pd.concat([datos, etiquetas], ignore_index=True)\n",
        "    i += 1\n",
        "\n",
        "  print('Data set creado de tamaño: ', datos.shape)\n",
        "\n",
        "  numeric_cols = ['instancia_id', 'token_id', 'punt_inicial', 'punt_final', 'capitalización']\n",
        "  for col in numeric_cols:\n",
        "    datos[col] = pd.to_numeric(datos[col], errors='coerce')\n",
        "\n",
        "  return parrafos, datos"
      ],
      "metadata": {
        "id": "-T2ifEiGsR8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def agregar_embeddings(df_X): #habria que sacar esta\n",
        "  token_ids = df_X['token_id'].tolist()\n",
        "  embeddings_list = []\n",
        "  for token_id  in token_ids:\n",
        "      if token_id is None or token_id == tokenizer.unk_token_id:\n",
        "        token_id = tokenizer.unk_token_id\n",
        "      # Detach the tensor before converting to list\n",
        "      embedding = model.embeddings.word_embeddings.weight[token_id].detach().tolist()\n",
        "      embeddings_list.append(embedding)\n",
        "  df_X['embeddings'] = embeddings_list\n",
        "  # The embeddings are already lists of floats, no need to convert to int\n",
        "  # df_X['embeddings'] = df_X['embeddings'].apply(lambda x: [int(i) for i in x])\n",
        "\n",
        "  return df_X"
      ],
      "metadata": {
        "id": "uzFovjOPsTOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trasformar_df_dfPyTorch(datos_X, datos_Y):\n",
        "  # Convert the list of lists in the 'embeddings' column to a NumPy array of floats\n",
        "  embeddings_array = np.array(datos_X['embeddings'].tolist(), dtype=np.float32)\n",
        "  X = torch.tensor(embeddings_array, dtype=torch.float32)\n",
        "  Y = torch.tensor(datos_Y[['punt_inicial', 'punt_final', 'capitalización']].values, dtype=torch.float32)\n",
        "  dataSetPT = TensorDataset(X, Y)\n",
        "\n",
        "  return dataSetPT"
      ],
      "metadata": {
        "id": "u2JtfEwIsfjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'https://raw.githubusercontent.com/AzulBarr/Aprendizaje-Automatico/main/TPs/tp2'\n"
      ],
      "metadata": {
        "id": "v6C-6HTBsg24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "libro1 = '/Harry_Potter_y_el_caliz_de_fuego_J_K_Rowling.epub'"
      ],
      "metadata": {
        "id": "_lMmvNJmsj_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = path + libro1\n"
      ],
      "metadata": {
        "id": "BhBsR_bcsmO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O libro1.epub $path\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2WkOAYegsoBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parrafos, dataSet = crearDataSet('libro1.epub')"
      ],
      "metadata": {
        "id": "n4DpCsmosqjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parrafos.head()"
      ],
      "metadata": {
        "id": "ywTmyS5UtJg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parrafos.iloc[100]"
      ],
      "metadata": {
        "id": "3UVnVim8ui_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataSet.loc[dataSet[\"instancia_id\"] == 100]"
      ],
      "metadata": {
        "id": "D2IJ6KQ2tUMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datos_X = dataSet[['instancia_id', 'token_id', 'token']].copy()\n",
        "datos_Y = dataSet[['punt_inicial', 'punt_final', 'capitalización']].copy()"
      ],
      "metadata": {
        "id": "NYInutFvuv1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datos_X_ext = agregar_embeddings(datos_X)"
      ],
      "metadata": {
        "id": "l4d0_xgMuyHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataSetPT = trasformar_df_dfPyTorch(datos_X, datos_Y)\n"
      ],
      "metadata": {
        "id": "oFyinnChu6Af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Padding"
      ],
      "metadata": {
        "id": "nZg_KvS8w3lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    batch: lista de tuplas (embeddings, labels)\n",
        "    \"\"\"\n",
        "    embeddings_list, labels_list = zip(*batch)\n",
        "\n",
        "    # Pad embeddings (seq_len, embedding_dim) -> (batch_size, max_seq_len, embedding_dim)\n",
        "    embeddings_padded = pad_sequence(embeddings_list, batch_first=True, padding_value=0.0)\n",
        "\n",
        "    # Pad labels\n",
        "    punt_inicial = pad_sequence([l[\"punt_inicial\"] for l in labels_list], batch_first=True, padding_value=-100)\n",
        "    punt_final = pad_sequence([l[\"punt_final\"] for l in labels_list], batch_first=True, padding_value=-100)\n",
        "    capitalizacion = pad_sequence([l[\"capitalización\"] for l in labels_list], batch_first=True, padding_value=-100)\n",
        "\n",
        "    return embeddings_padded, {\n",
        "        \"punt_inicial\": punt_inicial,\n",
        "        \"punt_final\": punt_final,\n",
        "        \"capitalizacion\": capitalizacion\n",
        "    }"
      ],
      "metadata": {
        "id": "f-Uh67l1w52p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loader"
      ],
      "metadata": {
        "id": "31pceL-iwfYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataSetPT, batch_size=8, shuffle=True, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "UKEFBQ8mwiOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelos"
      ],
      "metadata": {
        "id": "ScpDtm8_omtl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Buscar atributos para Random Forest"
      ],
      "metadata": {
        "id": "2VmHjA5BsOr6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest"
      ],
      "metadata": {
        "id": "J4PW-sznqclR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score,confusion_matrix\n"
      ],
      "metadata": {
        "id": "hlzn7DzvxLqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download es_core_news_sm"
      ],
      "metadata": {
        "id": "IhM1rfW52-L1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "def categoria_gramatical(palabra):\n",
        "    doc = nlp(palabra)\n",
        "    token = doc[0]\n",
        "    return token.pos_   # etiqueta POS estándar\n",
        "\n",
        "# Ejemplos\n",
        "print(categoria_gramatical(\"hola\"))     # VERB\n",
        "print(categoria_gramatical(\"gato\"))       # NOUN\n",
        "print(categoria_gramatical(\"Azul\"))       # PROPN (sustantivo propio)\n",
        "print(categoria_gramatical(\"rápidamente\"))# ADV"
      ],
      "metadata": {
        "id": "TmtPBpiT3DuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verbo, sustantivo, sustantivo propio y adverbio"
      ],
      "metadata": {
        "id": "QUywDG3x4KmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanza"
      ],
      "metadata": {
        "id": "sd2PxLYg4QuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "stanza.download(\"es\")"
      ],
      "metadata": {
        "id": "H5gFJBig4TNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = stanza.Pipeline(\"es\", processors=\"tokenize,pos\")"
      ],
      "metadata": {
        "id": "sx46K3tX4Xpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def categoria_gramatical_stanza(palabra):\n",
        "    doc = nlp(palabra)\n",
        "    token = doc.sentences[0].words[0]\n",
        "    return token.upos  # etiqueta universal POS (NOUN, VERB, PROPN, etc.)"
      ],
      "metadata": {
        "id": "jgn5d_aP4a_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categoria_gramatical_stanza(\"j\")"
      ],
      "metadata": {
        "id": "dXww1Qhe4jp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "upos2id = {\n",
        "    \"NOUN\": 0,\n",
        "    \"PROPN\": 1,\n",
        "    \"VERB\": 2,\n",
        "    \"ADJ\": 3,\n",
        "    \"ADV\": 4,\n",
        "    \"PRON\": 5,\n",
        "    \"DET\": 6,\n",
        "    \"ADP\": 7,\n",
        "    \"SCONJ\": 8,\n",
        "    \"CCONJ\": 9,\n",
        "    \"NUM\": 10,\n",
        "    \"INTJ\": 11,\n",
        "    \"PART\": 12,\n",
        "    \"AUX\": 13,\n",
        "    \"PUNCT\": 14,\n",
        "    \"SYM\": 15,\n",
        "    \"X\": 16\n",
        "}\n",
        "\n",
        "def indice_categoria_stanza(palabra):\n",
        "    pos = categoria_gramatical_stanza(palabra)\n",
        "    return upos2id.get(pos,-1)"
      ],
      "metadata": {
        "id": "A82N12cl77d3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(\"k\")"
      ],
      "metadata": {
        "id": "5yQLl2feO24O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_set_RF = pd.DataFrame(columns = ['token_id','posicion_frase','categoria_gramatical','distancia_al_final','id_anterior',\"id_siguiente\",'es_principio','es_medio','es_final','forma_parte'] )"
      ],
      "metadata": {
        "id": "nvlZZiTI-Oe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_set_RF"
      ],
      "metadata": {
        "id": "esvKvuyc_LIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "'''\n",
        "\n",
        "Para cada parrafo, agarramos cada palabra de ese parrafo, primero hacemos lo de categorizar, tokenizamos\n",
        "Y a partir de esto armamos el nuevo dataset para RF.\n",
        "\n",
        "Dataset: token_id, posicion_frase, categoria gramatical {1,2,3,4,..}, distancia al final de la frase, token_id_palabra anterior , token_id palabra siguinte,\n",
        "si el token pertenece a otra palabra, principio, medio, final\n",
        "\n",
        "'''\n",
        "token_id = []\n",
        "posicion_frase = []\n",
        "categoria_gramatical = []\n",
        "distancia_al_final = []\n",
        "id_anterior = []\n",
        "id_siguiente = []\n",
        "es_principio = []\n",
        "es_medio = []\n",
        "es_final = []\n",
        "forma_parte = []\n",
        "\n",
        "for parrafo in parrafos['limpio'][:3]:\n",
        "\n",
        "  token_siguiente = -1\n",
        "  token_anterior = -1\n",
        "\n",
        "  for i,palabra in enumerate(parrafo.split()):\n",
        "    categoria = indice_categoria_stanza(palabra)\n",
        "\n",
        "    tokens_id = tokenizer(palabra)['input_ids']\n",
        "\n",
        "    for j,id in enumerate(tokens_id):\n",
        "      token_id.append(id)\n",
        "      posicion_frase.append(i)\n",
        "      categoria_gramatical.append(categoria)\n",
        "      distancia_al_final.append(len(parrafo) - i)\n",
        "      id_anterior.append(token_anterior)\n",
        "      if j != len(tokens_id) - 1:\n",
        "        id_sig  = tokens_id[j + 1]\n",
        "        id_siguiente.append(id_sig)\n",
        "      else:\n",
        "        if i != len(parrafo) - 1:\n",
        "          token_siguiente = tokenizer(parrafo[i + 1])['input_ids'][0]\n",
        "          id_siguiente.append(token_siguiente)\n",
        "        else:\n",
        "          id_siguiente.append(-1)\n",
        "      es_medio_id = 0\n",
        "      es_final_id = 0\n",
        "      es_principio_id = 0\n",
        "\n",
        "      if j == 0:\n",
        "        es_principio_id = 1\n",
        "\n",
        "      elif j == len(tokens_id) - 1:\n",
        "        es_final_id = 1\n",
        "      else:\n",
        "        es_medio_id = 1\n",
        "\n",
        "      es_principio.append(es_principio_id)\n",
        "      es_medio.append(es_medio_id)\n",
        "      es_final.append(es_final_id)\n",
        "\n",
        "      token_anterior = id\n",
        "\n",
        "      forma_parte_id = 0\n",
        "\n",
        "      if j != 0:\n",
        "        forma_parte_id = 1\n",
        "\n",
        "      forma_parte.append(forma_parte_id)\n",
        "\n",
        "data_set_RF['token_id'] = token_id\n",
        "data_set_RF['posicion_frase'] = posicion_frase\n",
        "data_set_RF['categoria_gramatical'] = categoria_gramatical\n",
        "data_set_RF['distancia_al_final'] = distancia_al_final\n",
        "data_set_RF['id_anterior'] = id_anterior\n",
        "data_set_RF['id_siguiente'] = id_siguiente\n",
        "data_set_RF['es_principio'] = es_principio\n",
        "data_set_RF['es_medio'] = es_medio\n",
        "data_set_RF['es_final'] = es_final\n",
        "data_set_RF['forma_parte'] = forma_parte\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QdHHfjDJ4_GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datos_Y['instancia_id'] = datos_X['instancia_id']\n",
        "\n",
        "datos_X[\"instancia_id\"] = pd.to_numeric(datos_X[\"instancia_id\"], errors=\"coerce\")\n",
        "datos_Y[\"instancia_id\"] = pd.to_numeric(datos_Y[\"instancia_id\"], errors=\"coerce\")\n",
        "\n",
        "X_train = datos_X[datos_X['instancia_id'] < 2955]\n",
        "y_train = datos_Y[datos_Y['instancia_id'] < 2955]\n",
        "\n",
        "X_test = datos_X[datos_X['instancia_id'] >= 2955]\n",
        "y_test = datos_Y[datos_Y['instancia_id'] >= 2955]\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "LA-bKg-lS5pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train['embeddings']"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rh1kNF7fygRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[\"embeddings_mean\"] = X_train[\"embeddings\"].apply(lambda x: np.mean(x))\n"
      ],
      "metadata": {
        "id": "qqsoPn2jzJL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test[\"embeddings_mean\"] = X_test[\"embeddings\"].apply(lambda x: np.mean(x))\n"
      ],
      "metadata": {
        "id": "0lZ5s8JZz4yR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
        "\n",
        "    #n_estimators cantidad de arboles\n",
        "    # max_depth altura maxima de cada uno\n"
      ],
      "metadata": {
        "id": "s-1g2bDCx3Sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "id": "wF-CGyJn2_j3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train[['embeddings_mean']], y_train.drop(columns=['instancia_id']))\n"
      ],
      "metadata": {
        "id": "GPgR1LkqyEDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test[['embeddings_mean']])\n"
      ],
      "metadata": {
        "id": "CWF43puSz8Rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(y_test['punt_inicial'], y_pred[:,0],average=\"macro\")"
      ],
      "metadata": {
        "id": "y2KfWJUV0bbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(y_test['punt_inicial'],y_pred[:,1],average = \"macro\")"
      ],
      "metadata": {
        "id": "sBu0bVfE3V8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(y_test['capitalización'],y_pred[:,2],average = \"macro\")"
      ],
      "metadata": {
        "id": "Bd0pipyx3ftv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN Unidireccional"
      ],
      "metadata": {
        "id": "CZ8fuRwIp1hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderUnidireccional(nn.Module):\n",
        "    def __init__(self, embedding_dim=768, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(EncoderUnidireccional, self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,#como es 2, significa que hay dos bloques de celdas LSTM\n",
        "            batch_first=True, #(batch, seq, feature)\n",
        "            dropout=dropout, #dropout probability\n",
        "            bidirectional=False  # unidireccional\n",
        "        )\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        \"\"\"\n",
        "        embeddings: tensor de forma (batch_size, seq_len, embedding_dim)\n",
        "        \"\"\"\n",
        "        outputs, (hidden, cell) = self.lstm(embeddings)\n",
        "        # outputs: (batch_size, seq_len, hidden_dim)\n",
        "        # hidden: (num_layers, batch_size, hidden_dim)\n",
        "        # cell:   (num_layers, batch_size, hidden_dim)\n",
        "        return outputs, (hidden, cell)\n"
      ],
      "metadata": {
        "id": "aauZ2P-3oonB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderUnidireccional(nn.Module):\n",
        "    def __init__(self, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(DecoderUnidireccional, self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=hidden_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=False  # unidireccional\n",
        "        )\n",
        "\n",
        "        # Capa feed-forward para cada problema\n",
        "        self.punt_inicial_ff = nn.Linear(hidden_dim, 2)\n",
        "        self.punt_final_ff = nn.Linear(hidden_dim, 4)\n",
        "        self.capital_ff = nn.Linear(hidden_dim, 4)\n",
        "\n",
        "        # Función de activación para cada problema\n",
        "        #self.punt_inicial_sigmoid = nn.Sigmoid()\n",
        "        #self.punt_final_softmax = nn.Softmax(dim=4)\n",
        "        #self.capital_softmax = nn.Softmax(dim=4)\n",
        "\n",
        "\n",
        "    def forward(self, encoder_outputs, hidden, cell):\n",
        "        \"\"\"\n",
        "        encoder_outputs: (batch_size, seq_len, hidden_dim)\n",
        "        hidden, cell: del encoder\n",
        "        \"\"\"\n",
        "        outputs, _ = self.lstm(encoder_outputs, (hidden, cell))\n",
        "\n",
        "        #punt_inicial_logits = self.punt_inicial_sigmoid(self.punt_inicial_ff(outputs))\n",
        "        #punt_final_logits = self.punt_final_sofmax(self.punt_final_ff(outputs))\n",
        "        #capital_logits = self.capital_sofmax(self.capital_ff(outputs))\n",
        "\n",
        "        punt_inicial_logits = self.punt_inicial_ff(outputs)\n",
        "        punt_final_logits = self.punt_final_ff(outputs)\n",
        "        capital_logits = self.capital_ff(outputs)\n",
        "\n",
        "\n",
        "        return {\n",
        "            \"puntuación inicial\": punt_inicial_logits,\n",
        "            \"puntuación final\": punt_final_logits,\n",
        "            \"capitalización\": capital_logits,\n",
        "        }"
      ],
      "metadata": {
        "id": "56o8-fDTpfbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encoder - Decoder"
      ],
      "metadata": {
        "id": "EH-ISLOVv2Pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModeloUnidireccional(nn.Module):\n",
        "    def __init__(self, embedding_dim=768, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(ModeloUnidireccional, self).__init__()\n",
        "        self.encoder = EncoderUnidireccional(embedding_dim, hidden_dim, num_layers, dropout)\n",
        "        self.decoder = DecoderUnidireccional(hidden_dim, num_layers, dropout)\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        encoder_outputs, (hidden, cell) = self.encoder(embeddings)\n",
        "        predictions = self.decoder(encoder_outputs, hidden, cell)\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "3ZoHpllJpyRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN Bidireccional"
      ],
      "metadata": {
        "id": "55DaPLK8p5VD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBidireccional(nn.Module):\n",
        "    def __init__(self, embedding_dim=768, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(EncoderBidireccional, self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=True  # bidireccional\n",
        "        )\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        \"\"\"\n",
        "        embeddings: tensor de forma (batch_size, seq_len, embedding_dim)\n",
        "        \"\"\"\n",
        "        outputs, (hidden, cell) = self.lstm(embeddings)\n",
        "        # outputs: (batch_size, seq_len, hidden_dim)\n",
        "        # hidden: (num_layers, batch_size, hidden_dim)\n",
        "        # cell:   (num_layers, batch_size, hidden_dim)\n",
        "        return outputs, (hidden, cell)"
      ],
      "metadata": {
        "id": "vNB4cjjAp4_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBidireccional(nn.Module):\n",
        "    def __init__(self, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(DecoderBidireccional, self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=hidden_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=True  # bidireccional\n",
        "        )\n",
        "\n",
        "        # Capa feed-forward para cada problema\n",
        "        self.punt_inicial_ff = nn.Linear(hidden_dim, 2)\n",
        "        self.punt_final_ff = nn.Linear(hidden_dim, 4)\n",
        "        self.capital_ff = nn.Linear(hidden_dim, 4)\n",
        "\n",
        "        # Función de activación para cada problema\n",
        "        #self.punt_inicial_sigmoid = nn.Sigmoid()\n",
        "        #self.punt_final_softmax = nn.Softmax(dim=4)\n",
        "        #self.capital_softmax = nn.Softmax(dim=4)\n",
        "\n",
        "\n",
        "    def forward(self, encoder_outputs, hidden, cell):\n",
        "        \"\"\"\n",
        "        encoder_outputs: (batch_size, seq_len, hidden_dim)\n",
        "        hidden, cell: del encoder\n",
        "        \"\"\"\n",
        "        outputs, _ = self.lstm(encoder_outputs, (hidden, cell))\n",
        "\n",
        "        #punt_inicial_logits = self.punt_inicial_sigmoid(self.punt_inicial_ff(outputs))\n",
        "        #punt_final_logits = self.punt_final_sofmax(self.punt_final_ff(outputs))\n",
        "        #capital_logits = self.capital_sofmax(self.capital_ff(outputs))\n",
        "\n",
        "        punt_inicial_logits = self.punt_inicial_ff(outputs)\n",
        "        punt_final_logits = self.punt_final_ff(outputs)\n",
        "        capital_logits = self.capital_ff(outputs)\n",
        "\n",
        "        return {\n",
        "            \"puntuación inicial\": punt_inicial_logits,\n",
        "            \"puntuación final\": punt_final_logits,\n",
        "            \"capitalización\": capital_logits,\n",
        "        }\n"
      ],
      "metadata": {
        "id": "AmRq0CiWwBCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encoder - Decoder bidireccional"
      ],
      "metadata": {
        "id": "-9h7wEuTwHG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModeloBidireccional(nn.Module):\n",
        "    def __init__(self, embedding_dim=768, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(ModeloBidireccional, self).__init__()\n",
        "        self.encoder = EncoderBidireccional(embedding_dim, hidden_dim, num_layers, dropout)\n",
        "        self.decoder = DecoderBidireccional(hidden_dim, num_layers, dropout)\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        encoder_outputs, (hidden, cell) = self.encoder(embeddings)\n",
        "        predictions = self.decoder(encoder_outputs, hidden, cell)\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "oadDYbuowGS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entrenamiento"
      ],
      "metadata": {
        "id": "TSWNUrgdwMqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ModeloUnidireccional(embedding_dim=768, hidden_dim=256, num_layers=2)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)  # ignorar padding\n",
        "num_epochs = 1\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for embeddings, labels in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(embeddings)  # diccionario con tus tres salidas\n",
        "\n",
        "        loss_inicial = criterion(outputs[\"puntuación inicial\"].permute(0,2,1), labels[\"punt_inicial\"])\n",
        "        loss_final = criterion(outputs[\"puntuación final\"].permute(0,2,1), labels[\"punt_final\"])\n",
        "        loss_cap = criterion(outputs[\"capitalización\"].permute(0,2,1), labels[\"capitalizacion\"])\n",
        "\n",
        "        loss = loss_inicial + loss_final + loss_cap\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "DfkgZ-QvwQFx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}