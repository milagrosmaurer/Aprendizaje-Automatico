{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "nZg_KvS8w3lw",
        "31pceL-iwfYz",
        "CZ8fuRwIp1hn",
        "55DaPLK8p5VD",
        "TSWNUrgdwMqI"
      ],
      "authorship_tag": "ABX9TyProsTtuMEnmQuchwCPS6xl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/milagrosmaurer/Aprendizaje-Automatico/blob/main/TP2/TP2_automatico.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Carga de librerias"
      ],
      "metadata": {
        "id": "H78qTC9BlZMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ebooklib beautifulsoup4 pandas\n",
        "!pip install stanza\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9Wct7dMGqvR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJ3TrxZflROM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset,Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from typing import List, Dict, Any\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ebooklib import epub\n",
        "import ebooklib\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import stanza\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score,confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"bert-base-multilingual-cased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "model.eval()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RKHUgiJplYsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stanza.download(\"es\")\n",
        "nlp = stanza.Pipeline(\"es\", processors=\"tokenize,pos\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_P6mDqPAla4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definición de funciones"
      ],
      "metadata": {
        "id": "AsBnofU0ldDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text_X(t):\n",
        "    # Convertir a minúsculas y quitar puntuación\n",
        "    t = t.lower()\n",
        "    t = re.sub(r'[\\u200b-\\u200f\\uFEFF]', '', t)\n",
        "    t = re.sub(r\"[^a-záéíóúüñ0-9' -]+\", ' ', t)\n",
        "    t = re.sub(r'\\s+', ' ', t).strip()\n",
        "    return t"
      ],
      "metadata": {
        "id": "abGSiELDq-AG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text_y(t):\n",
        "    # Convertir a minúsculas y quitar puntuación\n",
        "    t = re.sub(r'[\\u200b-\\u200f\\uFEFF]', '', t)\n",
        "    t = re.sub(r\"[^a-zA-Záéíóúüñ0-9¿?,.' -]+\", ' ', t)\n",
        "    t = re.sub(r'\\s+', ' ', t).strip()\n",
        "    return t"
      ],
      "metadata": {
        "id": "L41yJLZ-rFBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prueba = \"Hola...cómo estás?\"\n",
        "\n",
        "print(prueba.lower())\n",
        "token_1 = tokenizer.tokenize(prueba.lower())\n",
        "print(token_1)\n",
        "print(tokenizer.convert_tokens_to_ids(token_1))\n",
        "prueba_sin_punt = normalize_text_X(prueba)\n",
        "print(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(prueba_sin_punt)))\n",
        "\n",
        "for palabra in prueba.split():\n",
        "  print(palabra)\n",
        "  print(tokenizer.tokenize(palabra.lower()))\n",
        "\n"
      ],
      "metadata": {
        "id": "-JEv5VHPa5vE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transformar_etiqueta(y, indice):\n",
        "  puntuacion_iniciales = []\n",
        "  puntuacion_finales = []\n",
        "  capitalizaciones = []\n",
        "  instancia_ids = []\n",
        "  token_ids = []\n",
        "  tokens_l = []\n",
        "\n",
        "  for parrafo in y:\n",
        "    inicio_pregunta = False\n",
        "    palabras = parrafo.split()\n",
        "\n",
        "\n",
        "    for palabra in palabras:\n",
        "      tokens = tokenizer.tokenize(palabra.lower())\n",
        "\n",
        "      for i in range(len(tokens)):\n",
        "        if tokens[i] == \"¿\":\n",
        "          inicio_pregunta = True\n",
        "          continue\n",
        "        if tokens[i] == \"?\" or tokens[i] == \".\" or tokens[i] == \",\":\n",
        "          continue\n",
        "\n",
        "        instancia_ids.append(indice)\n",
        "        token_ids.append(tokenizer.convert_tokens_to_ids(tokens[i]))\n",
        "        tokens_l.append(tokens[i])\n",
        "\n",
        "        if inicio_pregunta:\n",
        "          puntuacion_iniciales.append(1) #('\"¿\"')\n",
        "          inicio_pregunta = False\n",
        "        else:\n",
        "          puntuacion_iniciales.append(0) #(\"\")\n",
        "\n",
        "        if i != len(tokens) - 1:\n",
        "\n",
        "          if tokens[i+1] == \"?\":\n",
        "            puntuacion_finales.append(3) #('\"?\"')\n",
        "          elif tokens[i+1] == \".\":\n",
        "            puntuacion_finales.append(1) #('\".\"')\n",
        "          elif tokens[i+1] == \",\":\n",
        "            puntuacion_finales.append(2) #('\",\"')\n",
        "          else:\n",
        "            puntuacion_finales.append(0) #(\"\")\n",
        "\n",
        "        else:\n",
        "          puntuacion_finales.append(0) #(\"\")\n",
        "\n",
        "        if palabra.islower():\n",
        "          capitalizaciones.append(0)\n",
        "          ultimo_numero = 0\n",
        "        elif palabra.istitle():\n",
        "          capitalizaciones.append(1)\n",
        "          ultimo_numero = 1\n",
        "        elif palabra.isupper():\n",
        "          capitalizaciones.append(3)\n",
        "          ultimo_numero = 3\n",
        "        else:\n",
        "          capitalizaciones.append(2)\n",
        "          ultimo_numero = 2\n",
        "\n",
        "  etiquetas = np.column_stack([instancia_ids, token_ids, tokens_l, puntuacion_iniciales, puntuacion_finales, capitalizaciones])\n",
        "  return etiquetas"
      ],
      "metadata": {
        "id": "JjpQPRzersC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convertir_epub_a_csv(archivo_epub='libro.epub'):\n",
        "  # Cargar el libro\n",
        "  book = ebooklib.epub.read_epub(archivo_epub)\n",
        "\n",
        "  # Lista donde se guardarán los párrafos\n",
        "  parrafos = []\n",
        "\n",
        "  # Recorremos los ítems del libro\n",
        "  for item in book.get_items():\n",
        "      if item.get_type() == ebooklib.ITEM_DOCUMENT:\n",
        "          # Parseamos el contenido HTML\n",
        "          soup = BeautifulSoup(item.get_body_content(), 'html.parser')\n",
        "          # Extraemos los párrafos\n",
        "          for p in soup.find_all('p'):\n",
        "            #print(\"p:\",p, 'tipo: ', type(p))\n",
        "            texto = p.get_text().strip()\n",
        "            #print(\"TEXTO:\",texto, ' tipo: ', type(texto))\n",
        "            palabras = texto.split()\n",
        "            #print(\"PALABRAS:\",palabras, ' tipo: ', type(palabras))\n",
        "            if len(palabras) < 20 or len(palabras) > 100:  # descartamos párrafos cortos\n",
        "                continue\n",
        "            if texto:\n",
        "                parrafos.append(texto)\n",
        "\n",
        "  df = pd.DataFrame({'parrafo': parrafos})\n",
        "  df.to_csv(\"libro_parrafos.csv\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "  print(f\"Se extrajeron {len(parrafos)} párrafos y se guardaron en 'libro_parrafos.csv'.\")"
      ],
      "metadata": {
        "id": "y48N-5J8s7We"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crearDataSet(libro):\n",
        "  convertir_epub_a_csv(libro)\n",
        "  df = pd.read_csv('libro_parrafos.csv')\n",
        "  parrafos = pd.DataFrame(columns=['default', 'limpio'])\n",
        "  parrafos['limpio'] = df['parrafo'].apply(normalize_text_X)\n",
        "  parrafos['default'] = df['parrafo'].apply(normalize_text_y)\n",
        "\n",
        "  columnas = ['instancia_id', 'token_id', 'token', 'punt_inicial', 'punt_final', 'capitalización']\n",
        "  datos = pd.DataFrame(columns=columnas)\n",
        "\n",
        "  i = 0\n",
        "  for p in parrafos['default']:\n",
        "    etiquetas = transformar_etiqueta([p], i)\n",
        "    etiquetas = pd.DataFrame(etiquetas, columns=columnas)\n",
        "    datos = pd.concat([datos, etiquetas], ignore_index=True)\n",
        "    i += 1\n",
        "\n",
        "  print('Data set creado de tamaño: ', datos.shape)\n",
        "\n",
        "  numeric_cols = ['instancia_id', 'token_id', 'punt_inicial', 'punt_final', 'capitalización']\n",
        "  for col in numeric_cols:\n",
        "    datos[col] = pd.to_numeric(datos[col], errors='coerce')\n",
        "\n",
        "  return parrafos, datos"
      ],
      "metadata": {
        "id": "-T2ifEiGsR8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def agregar_embeddings(df_X): #habria que sacar esta\n",
        "  token_ids = df_X['token_id'].tolist()\n",
        "  embeddings_list = []\n",
        "  for token_id  in token_ids:\n",
        "      if token_id is None or token_id == tokenizer.unk_token_id:\n",
        "        token_id = tokenizer.unk_token_id\n",
        "      # Detach the tensor before converting to list\n",
        "      embedding = model.embeddings.word_embeddings.weight[token_id].detach().tolist()\n",
        "      embeddings_list.append(embedding)\n",
        "  df_X['embeddings'] = embeddings_list\n",
        "  # The embeddings are already lists of floats, no need to convert to int\n",
        "  # df_X['embeddings'] = df_X['embeddings'].apply(lambda x: [int(i) for i in x])\n",
        "\n",
        "  return df_X"
      ],
      "metadata": {
        "id": "uzFovjOPsTOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trasformar_df_dfPyTorch(datos_X, datos_Y):\n",
        "  # Convert the list of lists in the 'embeddings' column to a NumPy array of floats\n",
        "  embeddings_array = np.array(datos_X['embeddings'].tolist(), dtype=np.float32)\n",
        "  X = torch.tensor(embeddings_array, dtype=torch.float32)\n",
        "  Y = torch.tensor(datos_Y[['punt_inicial', 'punt_final', 'capitalización']].values, dtype=torch.float32)\n",
        "  dataSetPT = TensorDataset(X, Y)\n",
        "\n",
        "  return dataSetPT"
      ],
      "metadata": {
        "id": "u2JtfEwIsfjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def categoria_gramatical_stanza(palabra):\n",
        "    doc = nlp(palabra)\n",
        "    token = doc.sentences[0].words[0]\n",
        "    return token.upos\n",
        "\n",
        "upos2id = {\n",
        "    \"NOUN\": 0, #Sustantivo común. Ej: gato, casa, libro, profesor\n",
        "    \"PROPN\": 1, #Sustantivo propio. Ej: Argentina, Azul, Google\n",
        "    \"VERB\": 2, #Verbo léxico. Ej: comer, hablar, correr\n",
        "    \"ADJ\": 3, #Adjetivo. Ej: rápido, azul, brillante\n",
        "    \"ADV\": 4, #Adverbio. Ej: rápidamente, muy, cerca\n",
        "    \"PRON\": 5, #Pronombre. Ej: yo, tú, él, eso, alguien\n",
        "    \"DET\": 6, #Determinante / artículo. Ej: el, la, los, un, ese, mi\n",
        "    \"ADP\": 7, #Adposición: preposición o posposición. Ej: de, para, con, sin, sobre\n",
        "    \"SCONJ\": 8, #Conjunción subordinante. Ej: que, porque, aunque, si\n",
        "    \"CCONJ\": 9, #Conjunción coordinante. Ej: y, o, pero, ni\n",
        "    \"NUM\": 10, #Numeral. Ej: uno, dos, 50, tercero\n",
        "    \"INTJ\": 11, #Interjección. Ej: ay!, hola!, uf, eh\n",
        "    \"PART\": 12, #Partícula gramatical (raro en español). Ejemplos típicos en inglés (not, 's), en español casi no se usa, pero aparece en casos como \"sí\" enfático.\n",
        "    \"AUX\": 13, #Verbo auxiliar. Ej: haber, ser (cuando forman tiempos compuestos: “he comido”, “está hablando”)\n",
        "    \"PUNCT\": 14, #Signos de puntuación. Ej: , . ; ! ?\n",
        "    \"SYM\": 15, #Símbolos. Ej: $, %, +, =, →\n",
        "    \"X\": 16 #Otros / desconocidos / extranjeros. Cualquier cosa que no encaja en ninguna categoría.\n",
        "}\n",
        "\n",
        "def indice_categoria_stanza(palabra):\n",
        "    pos = categoria_gramatical_stanza(palabra)\n",
        "    return upos2id.get(pos,-1)"
      ],
      "metadata": {
        "id": "qN9P98Trl6MS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crearDataSetRF(datos):\n",
        "  data_set_RF = pd.DataFrame(columns = ['token_id', 'posicion_frase',\n",
        "                                        'categoria_gramatical', 'distancia_al_final',\n",
        "                                        'id_anterior', 'id_siguiente', 'es_principio',\n",
        "                                        'es_medio', 'es_final', 'forma_parte'])\n",
        "\n",
        "  token_id = []\n",
        "  posicion_frase = []\n",
        "  categoria_gramatical = []\n",
        "  distancia_al_final = []\n",
        "  id_anterior = []\n",
        "  id_siguiente = []\n",
        "  es_principio = []\n",
        "  es_medio = []\n",
        "  es_final = []\n",
        "  forma_parte = []\n",
        "\n",
        "  for parrafo in datos:\n",
        "\n",
        "    token_siguiente = -1\n",
        "    token_anterior = -1\n",
        "    palabras = parrafo.split()\n",
        "\n",
        "    for i, palabra in enumerate(palabras):\n",
        "      categoria = indice_categoria_stanza(palabra)\n",
        "      tokens_id = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(palabra))\n",
        "\n",
        "      for j, id in enumerate(tokens_id):\n",
        "        token_id.append(id)\n",
        "        posicion_frase.append(i)\n",
        "        categoria_gramatical.append(categoria)\n",
        "        distancia_al_final.append(len(palabras) - i)\n",
        "        id_anterior.append(token_anterior)\n",
        "\n",
        "        n_tok = len(tokens_id)\n",
        "        if j != n_tok - 1:\n",
        "          id_sig  = tokens_id[j + 1]\n",
        "          id_siguiente.append(id_sig)\n",
        "        else:\n",
        "          if i != len(palabras) - 1:\n",
        "            token_siguiente = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(palabras[i + 1]))[0]\n",
        "            id_siguiente.append(token_siguiente)\n",
        "          else:\n",
        "            id_siguiente.append(-1)\n",
        "\n",
        "        es_medio_id = 0\n",
        "        es_final_id = 0\n",
        "        es_principio_id = 0\n",
        "        if j == 0:\n",
        "          es_principio_id = 1\n",
        "        elif j == n_tok - 1:\n",
        "          es_final_id = 1\n",
        "        else:\n",
        "          es_medio_id = 1\n",
        "        es_principio.append(es_principio_id)\n",
        "        es_medio.append(es_medio_id)\n",
        "        es_final.append(es_final_id)\n",
        "\n",
        "        token_anterior = id\n",
        "\n",
        "        forma_parte_id = 0\n",
        "        if n_tok != 1:\n",
        "          forma_parte_id = 1\n",
        "        forma_parte.append(forma_parte_id)\n",
        "\n",
        "  data_set_RF['token_id'] = token_id\n",
        "  data_set_RF['posicion_frase'] = posicion_frase\n",
        "  data_set_RF['categoria_gramatical'] = categoria_gramatical\n",
        "  data_set_RF['distancia_al_final'] = distancia_al_final\n",
        "  data_set_RF['id_anterior'] = id_anterior\n",
        "  data_set_RF['id_siguiente'] = id_siguiente\n",
        "  data_set_RF['es_principio'] = es_principio\n",
        "  data_set_RF['es_medio'] = es_medio\n",
        "  data_set_RF['es_final'] = es_final\n",
        "  data_set_RF['forma_parte'] = forma_parte\n",
        "\n",
        "  return data_set_RF"
      ],
      "metadata": {
        "id": "697qgzMcmG-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prueba texto corto"
      ],
      "metadata": {
        "id": "l7DiLQfEDXlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parrafo_prueba = [\n",
        "    \"La luna asomaba detrás de las nubes cuando Clara decidió salir a caminar. La calle estaba desierta y el silencio le resultaba casi reconfortante.\",\n",
        "\n",
        "    \"El viejo cuaderno tenía páginas sueltas y esquinas dobladas. Cada anotación parecía escrita por alguien distinto, como si varias voces intentaran hablar a la vez.\",\n",
        "\n",
        "    \"El viento golpeó las ventanas con una fuerza repentina. Tomás se levantó sobresaltado, preguntándose si el sonido venía realmente de afuera.\",\n",
        "\n",
        "    \"Había algo en el brillo del objeto que no parecía natural. Un matiz rojizo que cambiaba apenas uno lo miraba directamente.\",\n",
        "\n",
        "    \"El tren avanzaba lentamente entre campos dorados. Julia observaba el paisaje con una mezcla de nostalgia y curiosidad.\",\n",
        "\n",
        "    \"La cafetería estaba casi vacía cuando él entró. El aroma a café recién molido lo envolvió de inmediato, trayéndole recuerdos difusos.\",\n",
        "\n",
        "    \"En el pasillo oscuro, un murmullo leve se repetía como un eco. Sofía dudó un segundo antes de avanzar, pero la intriga pudo más.\",\n",
        "\n",
        "    \"El reloj marcaba la medianoche cuando la luz se apagó de golpe. Por un instante, todo quedó suspendido en una quietud incómoda.\",\n",
        "\n",
        "    \"Había leído ese mensaje tres veces y aún no podía descifrar su intención. ¿Era una advertencia, un pedido o simplemente un error?\",\n",
        "\n",
        "    \"La ciudad brillaba desde la terraza. Miles de luces formaban un paisaje que parecía vivo, respirando en ritmos propios.\"\n",
        "]\n",
        "\n",
        "\n",
        "df = pd.DataFrame({'parrafo': parrafo_prueba})\n",
        "\n",
        "\n",
        "parrafos_prueba = pd.DataFrame(columns=['default', 'limpio'])\n",
        "parrafos_prueba['limpio'] = df['parrafo'].apply(normalize_text_X)\n",
        "parrafos_prueba['default'] = df['parrafo'].apply(normalize_text_y)\n",
        "\n",
        "columnas = ['instancia_id', 'token_id', 'token', 'punt_inicial', 'punt_final', 'capitalización']\n",
        "datos = pd.DataFrame(columns=columnas)\n",
        "\n",
        "i = 0\n",
        "for p in parrafos_prueba['default']:\n",
        "  etiquetas = transformar_etiqueta([p], i)\n",
        "  etiquetas = pd.DataFrame(etiquetas, columns=columnas)\n",
        "  datos = pd.concat([datos, etiquetas], ignore_index=True)\n",
        "  i += 1\n",
        "\n",
        "print('Data set creado de tamaño: ', datos.shape)\n",
        "\n",
        "numeric_cols = ['instancia_id', 'token_id', 'punt_inicial', 'punt_final', 'capitalización']\n",
        "for col in numeric_cols:\n",
        "  datos[col] = pd.to_numeric(datos[col], errors='coerce')"
      ],
      "metadata": {
        "id": "hlxbTu48DZfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datos_RF = crearDataSetRF(parrafos_prueba['limpio'])"
      ],
      "metadata": {
        "id": "t-96oPsuFoBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datos_RF.shape"
      ],
      "metadata": {
        "id": "sUSEXuztFqJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0JQDZdYkEbTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carga de datos y creación del dataset"
      ],
      "metadata": {
        "id": "mokO_ecCmJGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'https://raw.githubusercontent.com/AzulBarr/Aprendizaje-Automatico/main/TPs/tp2'\n",
        "libro1 = '/Harry_Potter_y_el_caliz_de_fuego_J_K_Rowling.epub'\n",
        "path = path + libro1\n"
      ],
      "metadata": {
        "id": "v6C-6HTBsg24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O libro1.epub $path\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2WkOAYegsoBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parrafos, dataSet = crearDataSet('libro1.epub')"
      ],
      "metadata": {
        "id": "n4DpCsmosqjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parrafos.head()"
      ],
      "metadata": {
        "id": "ywTmyS5UtJg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataSet.head()"
      ],
      "metadata": {
        "id": "3UVnVim8ui_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datos_X = dataSet[['instancia_id', 'token_id', 'token']].copy()\n",
        "datos_Y = dataSet[['punt_inicial', 'punt_final', 'capitalización']].copy()"
      ],
      "metadata": {
        "id": "NYInutFvuv1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datos_X_ext = agregar_embeddings(datos_X)"
      ],
      "metadata": {
        "id": "l4d0_xgMuyHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataSetPT = trasformar_df_dfPyTorch(datos_X, datos_Y)\n"
      ],
      "metadata": {
        "id": "oFyinnChu6Af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Atributos para el dataset"
      ],
      "metadata": {
        "id": "ZFHCtOn3nO5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataSet_RF = pd.read_csv(\"dataSetRFSinEtiquetas.csv\")"
      ],
      "metadata": {
        "id": "rVyZo4vTneF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataSet.shape"
      ],
      "metadata": {
        "id": "6xtk-MUTnm5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataSet_RF.shape"
      ],
      "metadata": {
        "id": "X_FyvBuSyEYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataSet_RF[198990:199000]"
      ],
      "metadata": {
        "id": "bNUpW-ORySKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataSet[199006:199016]"
      ],
      "metadata": {
        "id": "eab1RDeVyZ3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Padding"
      ],
      "metadata": {
        "id": "nZg_KvS8w3lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    batch: lista de tuplas (embeddings, labels)\n",
        "    \"\"\"\n",
        "    embeddings_list, labels_list = zip(*batch)\n",
        "\n",
        "    # Pad embeddings (seq_len, embedding_dim) -> (batch_size, max_seq_len, embedding_dim)\n",
        "    embeddings_padded = pad_sequence(embeddings_list, batch_first=True, padding_value=0.0)\n",
        "\n",
        "    # Pad labels\n",
        "    punt_inicial = pad_sequence([l[\"punt_inicial\"] for l in labels_list], batch_first=True, padding_value=-100)\n",
        "    punt_final = pad_sequence([l[\"punt_final\"] for l in labels_list], batch_first=True, padding_value=-100)\n",
        "    capitalizacion = pad_sequence([l[\"capitalización\"] for l in labels_list], batch_first=True, padding_value=-100)\n",
        "\n",
        "    return embeddings_padded, {\n",
        "        \"punt_inicial\": punt_inicial,\n",
        "        \"punt_final\": punt_final,\n",
        "        \"capitalizacion\": capitalizacion\n",
        "    }"
      ],
      "metadata": {
        "id": "f-Uh67l1w52p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loader"
      ],
      "metadata": {
        "id": "31pceL-iwfYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataSetPT, batch_size=8, shuffle=True, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "UKEFBQ8mwiOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelos"
      ],
      "metadata": {
        "id": "ScpDtm8_omtl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Buscar atributos para Random Forest"
      ],
      "metadata": {
        "id": "2VmHjA5BsOr6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ],
      "metadata": {
        "id": "J4PW-sznqclR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Etiqueta 1, capitalización"
      ],
      "metadata": {
        "id": "JuZ8BppagRch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f, c = dataSet_RF.shape\n",
        "f2, c2 = dataSet.shape\n",
        "#dataSet_RF['capitalizacion'] = dataSet[:f]['capitalización']"
      ],
      "metadata": {
        "id": "0tBH4CXuglLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Length of dataSet['token_id']: {len(dataSet['token_id'])}\")\n",
        "print(f\"Length of dataSet_RF['token_id']: {len(dataSet_RF['token_id'])}\")\n",
        "\n",
        "unique_tokens_dataSet = set(dataSet['token_id'].unique())\n",
        "unique_tokens_dataSet_RF = set(dataSet_RF['token_id'].unique())\n",
        "\n",
        "diff_only_in_dataSet = unique_tokens_dataSet - unique_tokens_dataSet_RF\n",
        "diff_only_in_dataSet_RF = unique_tokens_dataSet_RF - unique_tokens_dataSet\n",
        "\n",
        "if not diff_only_in_dataSet and not diff_only_in_dataSet_RF:\n",
        "    print(\"\\nBoth series contain the same unique token_ids, although their lengths might differ.\")\n",
        "else:\n",
        "    if diff_only_in_dataSet:\n",
        "        print(f\"\\nToken_ids present in dataSet but not in dataSet_RF (first 10): {list(diff_only_in_dataSet)[:10]}\")\n",
        "        print(diff_only_in_dataSet)\n",
        "    if diff_only_in_dataSet_RF:\n",
        "        print(f\"\\nToken_ids present in dataSet_RF but not in dataSet (first 10): {list(diff_only_in_dataSet_RF)[:10]}\")\n",
        "        print(diff_only_in_dataSet_RF)\n",
        "\n",
        "lista_diff_only_dataSet = list(diff_only_in_dataSet)\n",
        "lista_diff_only_dataSetRF = list(diff_only_in_dataSet_RF)\n",
        "\n",
        "\n",
        "for token_id in lista_diff_only_dataSet:\n",
        "  print(tokenizer.convert_ids_to_tokens([token_id]))\n",
        "for token_id in lista_diff_only_dataSetRF:\n",
        "  print(tokenizer.convert_ids_to_tokens([token_id]))"
      ],
      "metadata": {
        "id": "p-Ua6h38gnY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lista_diff_only_dataSet[0]"
      ],
      "metadata": {
        "id": "YddkZ74BjUe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataSet[dataSet['token_id'] == lista_diff_only_dataSet[0]]"
      ],
      "metadata": {
        "id": "JHR9f3t5jX3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataSet[dataSet['token_id'] == lista_diff_only_dataSet[1]]"
      ],
      "metadata": {
        "id": "eLIwswtSkMUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataSet_RF[dataSet_RF['token_id'] == lista_diff_only_dataSetRF[1]]"
      ],
      "metadata": {
        "id": "JYq2TTaCj1aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataSet_RF[dataSet_RF['token_id'] == lista_diff_only_dataSetRF[0]]"
      ],
      "metadata": {
        "id": "Ohdmq_V2je9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN Unidireccional"
      ],
      "metadata": {
        "id": "CZ8fuRwIp1hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderUnidireccional(nn.Module):\n",
        "    def __init__(self, embedding_dim=768, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(EncoderUnidireccional, self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,#como es 2, significa que hay dos bloques de celdas LSTM\n",
        "            batch_first=True, #(batch, seq, feature)\n",
        "            dropout=dropout, #dropout probability\n",
        "            bidirectional=False  # unidireccional\n",
        "        )\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        \"\"\"\n",
        "        embeddings: tensor de forma (batch_size, seq_len, embedding_dim)\n",
        "        \"\"\"\n",
        "        outputs, (hidden, cell) = self.lstm(embeddings)\n",
        "        # outputs: (batch_size, seq_len, hidden_dim)\n",
        "        # hidden: (num_layers, batch_size, hidden_dim)\n",
        "        # cell:   (num_layers, batch_size, hidden_dim)\n",
        "        return outputs, (hidden, cell)\n"
      ],
      "metadata": {
        "id": "aauZ2P-3oonB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderUnidireccional(nn.Module):\n",
        "    def __init__(self, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(DecoderUnidireccional, self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=hidden_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=False  # unidireccional\n",
        "        )\n",
        "\n",
        "        # Capa feed-forward para cada problema\n",
        "        self.punt_inicial_ff = nn.Linear(hidden_dim, 2)\n",
        "        self.punt_final_ff = nn.Linear(hidden_dim, 4)\n",
        "        self.capital_ff = nn.Linear(hidden_dim, 4)\n",
        "\n",
        "        # Función de activación para cada problema\n",
        "        #self.punt_inicial_sigmoid = nn.Sigmoid()\n",
        "        #self.punt_final_softmax = nn.Softmax(dim=4)\n",
        "        #self.capital_softmax = nn.Softmax(dim=4)\n",
        "\n",
        "\n",
        "    def forward(self, encoder_outputs, hidden, cell):\n",
        "        \"\"\"\n",
        "        encoder_outputs: (batch_size, seq_len, hidden_dim)\n",
        "        hidden, cell: del encoder\n",
        "        \"\"\"\n",
        "        outputs, _ = self.lstm(encoder_outputs, (hidden, cell))\n",
        "\n",
        "        #punt_inicial_logits = self.punt_inicial_sigmoid(self.punt_inicial_ff(outputs))\n",
        "        #punt_final_logits = self.punt_final_sofmax(self.punt_final_ff(outputs))\n",
        "        #capital_logits = self.capital_sofmax(self.capital_ff(outputs))\n",
        "\n",
        "        punt_inicial_logits = self.punt_inicial_ff(outputs)\n",
        "        punt_final_logits = self.punt_final_ff(outputs)\n",
        "        capital_logits = self.capital_ff(outputs)\n",
        "\n",
        "\n",
        "        return {\n",
        "            \"puntuación inicial\": punt_inicial_logits,\n",
        "            \"puntuación final\": punt_final_logits,\n",
        "            \"capitalización\": capital_logits,\n",
        "        }"
      ],
      "metadata": {
        "id": "56o8-fDTpfbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encoder - Decoder"
      ],
      "metadata": {
        "id": "EH-ISLOVv2Pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModeloUnidireccional(nn.Module):\n",
        "    def __init__(self, embedding_dim=768, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(ModeloUnidireccional, self).__init__()\n",
        "        self.encoder = EncoderUnidireccional(embedding_dim, hidden_dim, num_layers, dropout)\n",
        "        self.decoder = DecoderUnidireccional(hidden_dim, num_layers, dropout)\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        encoder_outputs, (hidden, cell) = self.encoder(embeddings)\n",
        "        predictions = self.decoder(encoder_outputs, hidden, cell)\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "3ZoHpllJpyRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN Bidireccional"
      ],
      "metadata": {
        "id": "55DaPLK8p5VD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBidireccional(nn.Module):\n",
        "    def __init__(self, embedding_dim=768, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(EncoderBidireccional, self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=True  # bidireccional\n",
        "        )\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        \"\"\"\n",
        "        embeddings: tensor de forma (batch_size, seq_len, embedding_dim)\n",
        "        \"\"\"\n",
        "        outputs, (hidden, cell) = self.lstm(embeddings)\n",
        "        # outputs: (batch_size, seq_len, hidden_dim)\n",
        "        # hidden: (num_layers, batch_size, hidden_dim)\n",
        "        # cell:   (num_layers, batch_size, hidden_dim)\n",
        "        return outputs, (hidden, cell)"
      ],
      "metadata": {
        "id": "vNB4cjjAp4_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBidireccional(nn.Module):\n",
        "    def __init__(self, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(DecoderBidireccional, self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=hidden_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=True  # bidireccional\n",
        "        )\n",
        "\n",
        "        # Capa feed-forward para cada problema\n",
        "        self.punt_inicial_ff = nn.Linear(hidden_dim, 2)\n",
        "        self.punt_final_ff = nn.Linear(hidden_dim, 4)\n",
        "        self.capital_ff = nn.Linear(hidden_dim, 4)\n",
        "\n",
        "        # Función de activación para cada problema\n",
        "        #self.punt_inicial_sigmoid = nn.Sigmoid()\n",
        "        #self.punt_final_softmax = nn.Softmax(dim=4)\n",
        "        #self.capital_softmax = nn.Softmax(dim=4)\n",
        "\n",
        "\n",
        "    def forward(self, encoder_outputs, hidden, cell):\n",
        "        \"\"\"\n",
        "        encoder_outputs: (batch_size, seq_len, hidden_dim)\n",
        "        hidden, cell: del encoder\n",
        "        \"\"\"\n",
        "        outputs, _ = self.lstm(encoder_outputs, (hidden, cell))\n",
        "\n",
        "        #punt_inicial_logits = self.punt_inicial_sigmoid(self.punt_inicial_ff(outputs))\n",
        "        #punt_final_logits = self.punt_final_sofmax(self.punt_final_ff(outputs))\n",
        "        #capital_logits = self.capital_sofmax(self.capital_ff(outputs))\n",
        "\n",
        "        punt_inicial_logits = self.punt_inicial_ff(outputs)\n",
        "        punt_final_logits = self.punt_final_ff(outputs)\n",
        "        capital_logits = self.capital_ff(outputs)\n",
        "\n",
        "        return {\n",
        "            \"puntuación inicial\": punt_inicial_logits,\n",
        "            \"puntuación final\": punt_final_logits,\n",
        "            \"capitalización\": capital_logits,\n",
        "        }\n"
      ],
      "metadata": {
        "id": "AmRq0CiWwBCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encoder - Decoder bidireccional"
      ],
      "metadata": {
        "id": "-9h7wEuTwHG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModeloBidireccional(nn.Module):\n",
        "    def __init__(self, embedding_dim=768, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(ModeloBidireccional, self).__init__()\n",
        "        self.encoder = EncoderBidireccional(embedding_dim, hidden_dim, num_layers, dropout)\n",
        "        self.decoder = DecoderBidireccional(hidden_dim, num_layers, dropout)\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        encoder_outputs, (hidden, cell) = self.encoder(embeddings)\n",
        "        predictions = self.decoder(encoder_outputs, hidden, cell)\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "oadDYbuowGS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entrenamiento"
      ],
      "metadata": {
        "id": "TSWNUrgdwMqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ModeloUnidireccional(embedding_dim=768, hidden_dim=256, num_layers=2)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)  # ignorar padding\n",
        "num_epochs = 1\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for embeddings, labels in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(embeddings)  # diccionario con tus tres salidas\n",
        "\n",
        "        loss_inicial = criterion(outputs[\"puntuación inicial\"].permute(0,2,1), labels[\"punt_inicial\"])\n",
        "        loss_final = criterion(outputs[\"puntuación final\"].permute(0,2,1), labels[\"punt_final\"])\n",
        "        loss_cap = criterion(outputs[\"capitalización\"].permute(0,2,1), labels[\"capitalizacion\"])\n",
        "\n",
        "        loss = loss_inicial + loss_final + loss_cap\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "DfkgZ-QvwQFx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}